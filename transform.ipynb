{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET liiklusonnetused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Juhtumi nr', 'Toimumisaeg', 'Isikuid', 'Hukkunuid', 'Vigastatuid',\n",
      "       'Sõidukeid', 'Aadress (PPA)', 'Maja nr (PPA)', 'Tänav (PPA)',\n",
      "       'Ristuv tänav (PPA)', 'Maakond (PPA)', 'Omavalitsus (PPA)',\n",
      "       'Asustus (PPA)', 'Asula', 'Liiklusõnnetuse liik [1]',\n",
      "       'Liiklusõnnetuse liik [3]', 'Kergliikurijuhi osalusel',\n",
      "       'Jalakäija osalusel', 'Kaassõitja osalusel',\n",
      "       'Maastikusõiduki juhi osalusel',\n",
      "       'Eaka (65+) mootorsõidukijuhi osalusel', 'Bussijuhi osalusel',\n",
      "       'Veoautojuhi osalusel', 'Ühissõidukijuhi osalusel',\n",
      "       'Sõiduautojuhi osalusel', 'Mootorratturi osalusel',\n",
      "       'Mopeedijuhi osalusel', 'Jalgratturi osalusel', 'Alaealise osalusel',\n",
      "       'Turvavarustust mitte kasutanud isiku osalusel',\n",
      "       'Esmase juhiloa omaniku osalusel', 'Mootorsõidukijuhi osalusel',\n",
      "       'Tüüpskeemi nr', 'Tüüpskeem [2]', 'Tee tüüp [1]', 'Tee tüüp [2]',\n",
      "       'Tee element [1]', 'Tee element [2]', 'Tee objekt [2]', 'Kurvilisus',\n",
      "       'Tee tasasus', 'Tee seisund', 'Teekate', 'Teekatte seisund [2]',\n",
      "       'Sõiduradade arv', 'Lubatud sõidukiirus (PPA)', 'Tee nr (PPA)',\n",
      "       'Tee km (PPA)', 'Ilmastik [1]', 'Valgustus [1]', 'Valgustus [2]',\n",
      "       'GPS X', 'GPS Y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_original = pd.read_csv('liiklusonnetused_until_2021.csv')\n",
    "print(df_original.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Juhtumi nr       Toimumisaeg Asula          Liiklusõnnetuse liik [3]  \\\n",
      "0  2000140000057  24.10.2014 08:45   JAH            Kokkupõrge jalakäijaga   \n",
      "1  2000140000067  24.10.2014 13:45   JAH  Kokkupõrge ees liikuva sõidukiga   \n",
      "2  2000140000123           8/11/14   JAH  Kokkupõrge ees liikuva sõidukiga   \n",
      "3  2000140000235  17.11.2014 17:32    EI  Kokkupõrge vastutuleva sõidukiga   \n",
      "4  2000150000442  28.04.2015 07:55   JAH      Kokkupõrge sõidukiga küljelt   \n",
      "\n",
      "       GPS X     GPS Y  \n",
      "0  6588678.0  542647.0  \n",
      "1  6589522.0  541467.0  \n",
      "2  6593961.0  547646.0  \n",
      "3  6569324.0  516628.0  \n",
      "4  6586430.0  541953.0  \n"
     ]
    }
   ],
   "source": [
    "#Keeping only neccesary columns\n",
    "columns_to_keep = ['Juhtumi nr', 'Toimumisaeg', 'Asula', 'Liiklusõnnetuse liik [3]','GPS X', 'GPS Y']\n",
    "df = df_original[columns_to_keep]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Juhtumi nr         Toimumisaeg Asula          Liiklusõnnetuse liik [3]  \\\n",
      "0  2000140000057 2014-10-24 08:45:00   JAH            Kokkupõrge jalakäijaga   \n",
      "1  2000140000067 2014-10-24 13:45:00   JAH  Kokkupõrge ees liikuva sõidukiga   \n",
      "3  2000140000235 2014-11-17 17:32:00    EI  Kokkupõrge vastutuleva sõidukiga   \n",
      "4  2000150000442 2015-04-28 07:55:00   JAH      Kokkupõrge sõidukiga küljelt   \n",
      "6  2210140001932 2014-10-24 14:00:00    EI  Kokkupõrge vastutuleva sõidukiga   \n",
      "\n",
      "       GPS X     GPS Y  Day  Month  Year  Hour  Minute  \n",
      "0  6588678.0  542647.0   24     10  2014     8      45  \n",
      "1  6589522.0  541467.0   24     10  2014    13      45  \n",
      "3  6569324.0  516628.0   17     11  2014    17      32  \n",
      "4  6586430.0  541953.0   28      4  2015     7      55  \n",
      "6  6575538.0  529034.0   24     10  2014    14       0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jarve\\AppData\\Local\\Temp\\ipykernel_12848\\48742817.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['Toimumisaeg'] = pd.to_datetime(df_clean['Toimumisaeg'], dayfirst=True)\n",
      "C:\\Users\\jarve\\AppData\\Local\\Temp\\ipykernel_12848\\48742817.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.loc[:, 'Day'] = df_clean['Toimumisaeg'].dt.day\n",
      "C:\\Users\\jarve\\AppData\\Local\\Temp\\ipykernel_12848\\48742817.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.loc[:, 'Month'] = df_clean['Toimumisaeg'].dt.month\n",
      "C:\\Users\\jarve\\AppData\\Local\\Temp\\ipykernel_12848\\48742817.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.loc[:, 'Year'] = df_clean['Toimumisaeg'].dt.year\n",
      "C:\\Users\\jarve\\AppData\\Local\\Temp\\ipykernel_12848\\48742817.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.loc[:, 'Hour'] = df_clean['Toimumisaeg'].dt.hour\n",
      "C:\\Users\\jarve\\AppData\\Local\\Temp\\ipykernel_12848\\48742817.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.loc[:, 'Minute'] = df_clean['Toimumisaeg'].dt.minute\n"
     ]
    }
   ],
   "source": [
    "#Delete all rows that does not have time\n",
    "num_rows = df.shape[0]\n",
    "df_clean = df[~df['Toimumisaeg'].str.contains(r'\\d+/\\d+/\\d{2}')]\n",
    "num_rows_new = df_clean.shape[0]\n",
    "#print(df_clean.head())\n",
    "#print(num_rows) \n",
    "#print(num_rows_new) #15702 rows originally and now 9322, its almost half the data :(\n",
    "\n",
    "# Convert \"Toimumisaeg\" column to datetime format\n",
    "df_clean['Toimumisaeg'] = pd.to_datetime(df_clean['Toimumisaeg'], dayfirst=True)\n",
    "#print(df_clean.head())\n",
    "\n",
    "# Extract day, month, year, hour, and minute into separate columns\n",
    "df_clean.loc[:, 'Toimumisaeg'] = pd.to_datetime(df_clean['Toimumisaeg'], dayfirst=True)\n",
    "df_clean.loc[:, 'Day'] = df_clean['Toimumisaeg'].dt.day\n",
    "df_clean.loc[:, 'Month'] = df_clean['Toimumisaeg'].dt.month\n",
    "df_clean.loc[:, 'Year'] = df_clean['Toimumisaeg'].dt.year\n",
    "df_clean.loc[:, 'Hour'] = df_clean['Toimumisaeg'].dt.hour\n",
    "df_clean.loc[:, 'Minute'] = df_clean['Toimumisaeg'].dt.minute\n",
    "\n",
    "\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Juhtumi_nr         Toimumisaeg Asula              Liiklusonnetuse_liik  \\\n",
      "0  2000140000057 2014-10-24 08:45:00   JAH            Kokkupõrge jalakäijaga   \n",
      "1  2000140000067 2014-10-24 13:45:00   JAH  Kokkupõrge ees liikuva sõidukiga   \n",
      "3  2000140000235 2014-11-17 17:32:00    EI  Kokkupõrge vastutuleva sõidukiga   \n",
      "4  2000150000442 2015-04-28 07:55:00   JAH      Kokkupõrge sõidukiga küljelt   \n",
      "6  2210140001932 2014-10-24 14:00:00    EI  Kokkupõrge vastutuleva sõidukiga   \n",
      "\n",
      "       GPS X     GPS Y  Day  Month  Year  Hour  Minute  \n",
      "0  6588678.0  542647.0   24     10  2014     8      45  \n",
      "1  6589522.0  541467.0   24     10  2014    13      45  \n",
      "3  6569324.0  516628.0   17     11  2014    17      32  \n",
      "4  6586430.0  541953.0   28      4  2015     7      55  \n",
      "6  6575538.0  529034.0   24     10  2014    14       0  \n"
     ]
    }
   ],
   "source": [
    "#CHANGING COLUMN NAMES\n",
    "\n",
    "# Create a dictionary to map the old column names to the new column names\n",
    "column_mapping = {\n",
    "    'Juhtumi nr': 'Juhtumi_nr',\n",
    "    'Liiklusõnnetuse liik [3]': 'Liiklusonnetuse_liik',\n",
    "}\n",
    "\n",
    "# Use the 'rename()' method to change the column names\n",
    "df_clean = df_clean.rename(columns=column_mapping)\n",
    "\n",
    "# Print the modified DataFrame with the new column names\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juhtumi nr                     0\n",
      "Toimumisaeg                    0\n",
      "Asula                          0\n",
      "Liiklusõnnetuse liik [3]       0\n",
      "GPS X                       2244\n",
      "GPS Y                       2249\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#MISSING VALUES\n",
    "# Check for missing values in the DataFrame\n",
    "missing_values = df_clean.isnull()\n",
    "\n",
    "missing_counts = df.isnull().sum()\n",
    "print(missing_counts) #2244-l real ei ole GPS koordinaate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liiklusonnetuse_liik\n",
      "Kokkupõrge jalakäijaga               2162\n",
      "Kokkupõrge sõidukiga küljelt         1814\n",
      "Sõiduki teelt väljasõit              1314\n",
      "Kokkupõrge vastutuleva sõidukiga      690\n",
      "Kokkupõrge ees liikuva sõidukiga      635\n",
      "Sõiduki ümberpaiskumine teel          632\n",
      "Kokkupõrge teevälise takistusega      567\n",
      "Muu liiklusõnnetus                    431\n",
      "Kokkupõrge ees seisva sõidukiga       361\n",
      "Kukkumine ühissõidukis                303\n",
      "Sõidukite külgkokkupõrge              174\n",
      "Kokkupõrge loomaga                    112\n",
      "Kokkupõrge teel oleva takistusega      74\n",
      "Teadmata                               53\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#LIIKLUSONNETUSE LIIK VALUES\n",
    "\n",
    "# Get the unique values and their counts in the 'Liiklusonnetuse_liik' column\n",
    "value_counts = df_clean['Liiklusonnetuse_liik'].value_counts()\n",
    "\n",
    "# Print the unique values and their counts\n",
    "print(value_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET liiklusloendus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jarve\\AppData\\Local\\Temp\\ipykernel_12848\\1483412548.py:10: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_liiklus = pd.read_csv(file)\n",
      "C:\\Users\\jarve\\AppData\\Local\\Temp\\ipykernel_12848\\1483412548.py:10: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_liiklus = pd.read_csv(file)\n",
      "C:\\Users\\jarve\\AppData\\Local\\Temp\\ipykernel_12848\\1483412548.py:10: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_liiklus = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1   2  3   4  5  6  7  8  9  10  ... 40-<50  50-<60 60-<70  70-<80  80-<90  \\\n",
      "0  0  39  0   7  0  0  0  0  0   0  ...      0       1      0       8      18   \n",
      "1  1  75  0  12  1  0  0  0  0   1  ...      0       2      4      11      43   \n",
      "2  0  54  0   3  0  0  0  0  0   0  ...      0       0      6       7      26   \n",
      "3  1  30  0   6  0  0  0  0  0   0  ...      0       4      2      12      15   \n",
      "4  0  26  0   1  0  0  1  0  0   0  ...      1       2      8       3       8   \n",
      "\n",
      "   90-<100  100-<110  110-<120  120-<130  =>130  \n",
      "0       16         3         0         0      0  \n",
      "1       26         2         0         1      1  \n",
      "2       18         0         0         0      0  \n",
      "3        3         1         0         0      0  \n",
      "4        5         0         0         0      0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# MAKING ONE TABLE OF FILES IN \"liiklusloendus\" FOLDER\n",
    "ll_combined = pd.DataFrame()\n",
    "\n",
    "folder_path = 'liiklusloendus'  # Replace with the actual folder path\n",
    "\n",
    "file_paths = glob.glob(folder_path + '/*.csv')  # Get all CSV file paths in the folder\n",
    "dfs = []\n",
    "\n",
    "for file in file_paths:\n",
    "    df_liiklus = pd.read_csv(file)\n",
    "    dfs.append(df_liiklus)\n",
    "\n",
    "ll_combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(ll_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'id', 'kanal', 'aeg',\n",
      "       '<40Kph', '40-<50', '50-<60', '60-<70', '70-<80', '80-<90', '90-<100',\n",
      "       '100-<110', '110-<120', '120-<130', '=>130'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(ll_combined.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.21 GiB for an array with shape (22, 7380548) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 18\u001b[0m\n\u001b[0;32m      4\u001b[0m column_mapping \u001b[39m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mMotorcycle\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mCar\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m10\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mBuss\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     17\u001b[0m \u001b[39m# Use the 'rename()' method to change the column names\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m ll_combined \u001b[39m=\u001b[39m ll_combined\u001b[39m.\u001b[39;49mrename(columns\u001b[39m=\u001b[39;49mcolumn_mapping)\n\u001b[0;32m     20\u001b[0m \u001b[39m# Print the modified DataFrame with the new column names\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(ll_combined\u001b[39m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\jarve\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5432\u001b[0m, in \u001b[0;36mDataFrame.rename\u001b[1;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[0;32m   5313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrename\u001b[39m(\n\u001b[0;32m   5314\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5315\u001b[0m     mapper: Renamer \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5323\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5324\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5325\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5326\u001b[0m \u001b[39m    Rename columns or index labels.\u001b[39;00m\n\u001b[0;32m   5327\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5430\u001b[0m \u001b[39m    4  3  6\u001b[39;00m\n\u001b[0;32m   5431\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5432\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_rename(\n\u001b[0;32m   5433\u001b[0m         mapper\u001b[39m=\u001b[39;49mmapper,\n\u001b[0;32m   5434\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5435\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5436\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5437\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   5438\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5439\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5440\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5441\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jarve\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:1007\u001b[0m, in \u001b[0;36mNDFrame._rename\u001b[1;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         index \u001b[39m=\u001b[39m mapper\n\u001b[0;32m   1006\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_inplace_and_allows_duplicate_labels(inplace)\n\u001b[1;32m-> 1007\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy(deep\u001b[39m=\u001b[39;49mcopy \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m using_copy_on_write())\n\u001b[0;32m   1009\u001b[0m \u001b[39mfor\u001b[39;00m axis_no, replacements \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m((index, columns)):\n\u001b[0;32m   1010\u001b[0m     \u001b[39mif\u001b[39;00m replacements \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jarve\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:6452\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6342\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   6343\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcopy\u001b[39m(\u001b[39mself\u001b[39m: NDFrameT, deep: bool_t \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   6344\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   6345\u001b[0m \u001b[39m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6346\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6450\u001b[0m \u001b[39m    dtype: object\u001b[39;00m\n\u001b[0;32m   6451\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6452\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mcopy(deep\u001b[39m=\u001b[39;49mdeep)\n\u001b[0;32m   6453\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6454\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcopy\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jarve\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:664\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    661\u001b[0m         res\u001b[39m.\u001b[39m_blklocs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blklocs\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    663\u001b[0m \u001b[39mif\u001b[39;00m deep:\n\u001b[1;32m--> 664\u001b[0m     res\u001b[39m.\u001b[39;49m_consolidate_inplace()\n\u001b[0;32m    665\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\jarve\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1829\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_consolidate_inplace\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1824\u001b[0m     \u001b[39m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1825\u001b[0m     \u001b[39m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1826\u001b[0m     \u001b[39m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1827\u001b[0m     \u001b[39m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1829\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m _consolidate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks)\n\u001b[0;32m   1830\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_consolidated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1831\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known_consolidated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jarve\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2272\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2270\u001b[0m new_blocks: \u001b[39mlist\u001b[39m[Block] \u001b[39m=\u001b[39m []\n\u001b[0;32m   2271\u001b[0m \u001b[39mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[39min\u001b[39;00m grouper:\n\u001b[1;32m-> 2272\u001b[0m     merged_blocks, _ \u001b[39m=\u001b[39m _merge_blocks(\n\u001b[0;32m   2273\u001b[0m         \u001b[39mlist\u001b[39;49m(group_blocks), dtype\u001b[39m=\u001b[39;49mdtype, can_consolidate\u001b[39m=\u001b[39;49m_can_consolidate\n\u001b[0;32m   2274\u001b[0m     )\n\u001b[0;32m   2275\u001b[0m     new_blocks \u001b[39m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2276\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\jarve\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2304\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     new_values \u001b[39m=\u001b[39m bvals2[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_concat_same_type(bvals2, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m   2303\u001b[0m argsort \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2304\u001b[0m new_values \u001b[39m=\u001b[39m new_values[argsort]\n\u001b[0;32m   2305\u001b[0m new_mgr_locs \u001b[39m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2307\u001b[0m bp \u001b[39m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.21 GiB for an array with shape (22, 7380548) and data type int64"
     ]
    }
   ],
   "source": [
    "#CHANGING COLUMN NAMES\n",
    "#SEDA EI SAA PRAEGU TEHA, SEST MEMORY ERROR\n",
    "\n",
    "# Create a dictionary to map the old column names to the new column names\n",
    "column_mapping = {\n",
    "    '1': 'Motorcycle',\n",
    "    '2': 'Car',\n",
    "    '3': 'Car+Trailer',\n",
    "    '4': 'Heavy_Van',\n",
    "    '5': 'Light_Goods',\n",
    "    '6': 'Truck',\n",
    "    '7': 'Truck+Trailer',\n",
    "    '8': 'Articulated_Vehicle',\n",
    "    '9': 'Minibus',\n",
    "    '10': 'Buss',\n",
    "}\n",
    "\n",
    "# Use the 'rename()' method to change the column names\n",
    "ll_combined = ll_combined.rename(columns=column_mapping)\n",
    "\n",
    "# Print the modified DataFrame with the new column names\n",
    "print(ll_combined.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
